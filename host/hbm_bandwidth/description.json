{
    "testinfo": {
        "profile": "no"
    }, 
    "ndevice": [
        "xilinx_u200_xdma", 
        "xilinx_u200_qdma", 
        "xilinx_u250_xdma", 
        "xilinx_u250_qdma", 
        "xilinx_vcu1525_dynamic", 
        "xilinx_aws-vu9p-f1-04261818", 
        "xilinx:u200:xdma", 
        "xilinx:u200:qdma", 
        "xilinx:u250:xdma", 
        "xilinx:u250:qdma", 
        "xilinx:vcu1525:dynamic", 
        "xilinx:aws-vu9p-f1-04261818:dynamic"
    ], 
    "name": "HBM Bandwidth", 
    "contributors": [
        {
            "url": "http://www.xilinx.com", 
            "group": "Xilinx"
        }
    ], 
    "launch": [
        {
            "cmd_args": "BUILD/krnl_vaddmul.xclbin", 
            "name": "generic launch for all flows"
        }
    ], 
    "platform_type": "pcie", 
    "host": {
        "compiler": {
            "sources": [
                "REPO_DIR/common/includes/xcl2"
            ], 
            "includepaths": [
                "REPO_DIR/common/includes/xcl2"
            ]
        }, 
        "host_exe": "host"
    }, 
    "runtime": [
        "OpenCL"
    ], 
    "containers": [
        {
            "accelerators": [
                {
                    "location": "src/krnl_vaddmul.cpp", 
                    "name": "krnl_vaddmul", 
                    "num_compute_units": "8"
                }
            ], 
	    "ldclflags": "--config connectivity.ini",
            "name": "krnl_vaddmul"
        }
    ], 
    "description": [
        "This is a HBM bandwidth check design. Design contains 8 compute units of a kernel which has access to all HBM banks (0:31). Host application allocate buffer into all HBM banks and run these 8 compute units concurrently and measure the overall bandwidth between Kernel and HBM Memory."
    ]
}
